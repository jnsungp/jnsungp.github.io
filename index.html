<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junsung Park</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Junsung Park
                </p>

                <p>
                  I am an undergraduate student in the 
                  <a href="https://ece.snu.ac.kr/en">Department of Electrical and Computer Engineering</a> 
                  at Seoul National University, with a minor in Mechanical Engineering. 
                  I conduct research in the 
                  advised by Prof. 
                  <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Songhwai Oh</a>.  
                  My research interests include robot learning, vision–language–action models, 
                  cross-embodiment policy transfer, and offline reinforcement learning.
                </p>

                <p style="text-align:center">
                  <a href="mailto:night1115@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                  <a href="data/JunsungPark_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/junsung-park">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jnsungp">Github</a>
                </p>
              </td>

              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/JunsungPark.jpg">
                  <img style="width:100%;max-width:100%;object-fit:cover; border-radius:50%;" 
                       alt="profile photo" 
                       src="images/JunsungPark.jpg">
                </a>
              </td>

            </tr>
          </tbody></table>


          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research aims to build robust and generalizable robot autonomy.  
                  I focus on cross-embodiment policy adaptation, multimodal representation learning, 
                  and Learning-Based Control
                </p>
              </td>
            </tr>
          </tbody></table>


          <!-- Paper 1 -->
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <img 
                src="images/bicql-thumbnail.png"
                style="width:100%;border-radius:10px;"
                alt="BiCQL thumbnail">
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.12345">
                <span class="papertitle">BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning</span>
              </a>
              <br>
              <strong>Junsung Park</strong>
              <br>
              <em>arXiv 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2502.12345">Paper</a>
              <p>
                A policy-free offline IRL framework that jointly optimizes rewards and conservative Q-functions 
                via bi-level optimization, improving robustness and expert-likeness under dataset shift.
              </p>
            </td>
          </tr>


          <!-- Paper 2 -->
          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <img 
                src="images/g1-modality.png" 
                style="width:100%;border-radius:10px;"
                alt="G1 Modality">
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">Modality-Augmented Fine-Tuning of Foundation Robot Policies for Unitree G1</span>
              </a>
              <br>
              <strong>Junsung Park</strong>
              <br>
              <em>In preparation (Target: RSS 2026)</em>
              <br>
              <a href="#">Paper (coming soon)</a>
              <p>
                A study on incorporating depth, binary contact, and 8D force feedback into 
                diffusion-based foundation robot policies for cross-embodiment manipulation on the Unitree G1.
              </p>
            </td>
          </tr>


        </td>
      </tr>
    </tbody></table>
  </body>
</html>
