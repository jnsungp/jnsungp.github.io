<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junsung Park</title>

    <meta name="author" content="Junsung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Header / Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align:center;">
                      Junsung Park
                    </p>

                    <p>
                      I am an undergraduate student at <strong>Seoul National University (SNU)</strong>,
                      majoring in <a href="https://ece.snu.ac.kr/en">Electrical and Computer Engineering</a> with a minor in Mechanical Engineering.
                    </p>
                    
                    <p>
                      Currently, I am a research intern at the 
                      <a href="https://rllab.snu.ac.kr/">Robot Learning Laboratory (RLLAB)</a>,
                      advised by Prof. <a href="https://scholar.google.com/citations?user=VEzNY_oAAAAJ&hl=ko">Songhwai Oh</a>.
                    </p>

                    <p>
                      My research goal is to build <strong>generalizable robot autonomy</strong> capable of 
                      <strong>robust interaction</strong> with humans and objects in dynamic, unstructured environments.
                    </p>

                    <p style="text-align:center">
                      <a href="mailto:night1115@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                      <a href="data/CV_JunsungPark.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/junsung-park-26a34b33b/">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://huggingface.co/jnsungp">HuggingFace</a>
                    </p>
                  </td>

                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/JunsungPark.jpg">
                      <img
                        style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;box-shadow:0 4px 8px rgba(0,0,0,0.2);"
                        alt="profile photo"
                        src="images/JunsungPark.jpg"
                        class="hoverZoomLink">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Interest -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research Interest</h2>
                    <p>
                      My primary interest lies in <strong>Robot Learning</strong>, utilizing <strong>Reinforcement Learning</strong> 
                      and <strong>Learning-based Control</strong> to build robust systems.
                      <br><br>
                      I explore <strong>Vision–Language–Action (VLA) models</strong> to enable generalizable autonomy,
                      with the ultimate goal of advancing <strong>Human-Robot Interaction</strong>.
                      I am also investigating <strong>Wearable Devices</strong> as a key interface to capture human intent.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Publications & Preprints -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Publications &amp; Preprints</h2>
                  </td>
                </tr>

                
                <!-- BiCQL-ML -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;object-fit: cover;border-radius:10px;margin-top:4px;" muted autoplay loop playsinline>
                          <source src="images/bicql.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://arxiv.org/abs/2511.22210">
                      <b>BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood IRL</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>IEEE MIT URTC, Poster Presentation</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/2511.22210">[arXiv]</a>
                    <p></p>
                    <p>
                      We propose a <strong>policy-free offline Inverse Reinforcement Learning (IRL)</strong> framework that 
                      resolves reward ambiguity and distribution shift through a bi-level optimization structure
                      coupling conservative Q-learning with reward inference.
                    </p>
                  </td>
                </tr>

                
                <!-- Modality-Augmented Fine-Tuning -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;object-fit: cover;border-radius:10px;margin-top:8px;" muted autoplay loop playsinline>
                          <source src="images/g1_inf.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://arxiv.org/abs/2512.01358">
                      <b>Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Working in Process, In Preparation</em>
                    <br>
                    <a href="https://arxiv.org/abs/2512.01358">[arXiv]</a>
                    <p></p>
                    <p>
                      We fine-tune Vision-Language-Action foundation policies using a modality-augmented approach that fuses proprioception, depth, and contact forces, achieving 94% zero-shot pick-and-place manipulation success on the Unitree G1.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            
            <!-- Projects -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Projects</h2>
                  </td>
                </tr>

                  
                <!-- Wearable Rehabilitation Glove -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;height: 160px;object-fit: cover;border-radius:10px;margin-top:0px;" muted autoplay loop playsinline>
                          <source src="images/wearable_hand.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <b>Wearable Rehabilitation Glove</b>
                    <br>
                    <strong>Junsung Park</strong>, Injae John, Sungjae Hwang, and <a href="https://scholar.google.com/citations?user=kBsTE7AAAAAJ&hl=ko" target="_blank" style="text-decoration:none; color:#000;"> <strong>Yong-Lae Park</strong>
                    <br>
                    <em><a href="https://softrobotics.snu.ac.kr/" target="_blank" style="text-decoration:none; color:#555;">Soft Robotics & Bionics Lab (SRBL), Seoul National University </em>, 2024
                    <p></p>
                    <p>
                      Developed a wearable rehabilitation glove for stroke patients, integrating a rack-and-pinion actuator, e-gain soft sensor, and IMU-based feedback control to provide adaptive thumb motion assistance.
                    </p>
                  </td>
                </tr>

                <!-- Unitree G1 Dataset -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <img src="images/g1_data.png"
                             style="width:140%;margin-top:8px;border-radius:10px;border:1px solid #EEE;"
                             alt="G1 DATA">
                      </div>
                    </div>
                  </td>

                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://huggingface.co/jnsungp">
                      <b>Unitree G1 Multimodal Manipulation Dataset</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Open Source Project &amp; Dataset</em>, 2025
                    <br>
                    <a href="https://huggingface.co/jnsungp">[Dataset]</a>
                    <p></p>
                    <p>
                      Built a large-scale multimodal dataset-generation pipeline integrating Robosuite, RoboCasa, and cuRobo, producing synchronized RGB-D, contact-force, and proprioceptive data for training foundation robot policies.
                    </p>
                  </td>
                </tr>

                <!-- G1 Retargeting Project -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;object-fit: cover;border-radius:10px;margin-top:4px;" muted autoplay loop playsinline>
                          <source src="images/g1_retarget.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <b>Retargeting Human Motion to Unitree G1</b>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Humanoid Imitation Learning Project</em>, 2025
                    <p></p>
                    <p>
                      Developed a whole-body imitation learning pipeline that retargets human–object interaction motions from the OMOMO Dataset to the Unitree G1 using SMPL-based preprocessing and gradient optimization.
                    </p>
                  </td>
                </tr>

                
                

                <!-- Active Rainwater Removal System -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <img src="images/rainwater_system.png"
                             style="width:140%;margin-top:10px;border-radius:10px;border:1px solid #EEE;"
                             alt="Active Rainwater Removal System">
                      </div>
                    </div>
                  </td>
                  
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <b>Active Rainwater Removal System</b>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Mechatronics Design Competition, Awarded</em>, 2023
                    <p></p>
                    <p>
                      Developed a centrifugal umbrella system that actively removes rainwater using a high-torque motor and rotational airflow. Led the system design and integration, achieving fast water removal, low noise, and portable operation.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:small;">
                      Template source: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
