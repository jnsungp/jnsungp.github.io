<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junsung Park</title>

    <meta name="author" content="Junsung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Header / Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align:center;">
                      Junsung Park
                    </p>

                    <p>
                      I am an undergraduate student at <strong>Seoul National University (SNU)</strong>, 
                      double majoring in <a href="https://ece.snu.ac.kr/en">Electrical and Computer Engineering</a> and Mechanical Engineering.
                    </p>
                    
                    <p>
                      Currently, I am a research intern at the 
                      <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Robot Learning Laboratory (RLLAB)</a>,
                      advised by Prof. <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Songhwai Oh</a>.
                    </p>

                    <p>
                      My research goal is to build <strong>generalizable robot autonomy</strong> capable of 
                      <strong>robust interaction</strong> with humans and objects in dynamic, unstructured environments.
                    </p>

                    <p style="text-align:center">
                      <a href="mailto:night1115@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                      <a href="data/JunsungPark_CV.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/junsung-park">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://github.com/jnsungp">Github</a>
                    </p>
                  </td>

                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/JunsungPark.jpg">
                      <img
                        style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;box-shadow:0 4px 8px rgba(0,0,0,0.2);"
                        alt="profile photo"
                        src="images/JunsungPark.jpg"
                        class="hoverZoomLink">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Interest -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research Interest</h2>
                    <p>
                      My primary interest lies in <strong>Robot Learning</strong>, utilizing <strong>Reinforcement Learning</strong> 
                      and <strong>Learning-based Control</strong> to build robust systems.
                      <br><br>
                      I explore <strong>Vision–Language–Action (VLA) models</strong> to enable generalizable autonomy, 
                      with the ultimate goal of advancing <strong>Human-Robot Interaction</strong>. In this context, I am actively 
                      investigating <strong>Wearable Devices</strong> as a key interface to capture human intent and facilitate seamless collaboration.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Publications & Preprints -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Publications &amp; Preprints</h2>
                  </td>
                </tr>

                <!-- BiCQL-ML -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;margin-top:10px;" muted autoplay loop playsinline>
                          <source src="images/BICQL.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://arxiv.org/abs/XXXXXXXX">
                      <b>BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood IRL</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>IEEE MIT URTC, Poster Presentation</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/XXXXXXXX">[arXiv]</a>
                    <!-- / <a href="https://your-project-page-url">[Project Page]</a> -->
                    <p></p>
                    <p>
                      We propose a <strong>policy-free offline Inverse Reinforcement Learning (IRL)</strong> framework that addresses
                      reward ambiguity and distribution shift. By formulating a bi-level optimization problem that jointly learns
                      a conservative Q-function and a reward function, BiCQL-ML improves robustness and expert-likeness in offline settings.
                    </p>
                  </td>
                </tr>

                <!-- Modality-Augmented Fine-Tuning -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <img src="images/g1-modality.png"
                             style="width:140%;margin-top:10px;border-radius:10px;border:1px solid #EEE;"
                             alt="G1 Modality">
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="#">
                      <b>Modality-Augmented Fine-Tuning of Foundation Robot Policies</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>In Preparation</em>
                    <br>
                    <p></p>
                    <p>
                      Investigating the efficacy of incorporating dense tactile and force feedback into diffusion-based policies. 
                      This work focuses on <strong>cross-embodiment adaptation</strong>, specifically transferring manipulation skills 
                      to the Unitree G1 humanoid via modality-augmented fine-tuning.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Projects -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Projects</h2>
                  </td>
                </tr>

                <!-- Unitree G1 Dataset -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:100%;margin-top:10px;" muted autoplay loop playsinline>
                          <source src="images/g1_dataset_demo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://huggingface.co/jnsungp">
                      <b>Unitree G1 Multimodal Manipulation Dataset</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Open Source Project &amp; Dataset</em>, 2024–
                    <br>
                    <a href="https://huggingface.co/jnsungp">[Dataset]</a> /
                    <a href="https://github.com/jnsungp">[Code]</a>
                    <p></p>
                    <p>
                      Engineered a large-scale multimodal dataset generation pipeline for the <strong>Unitree G1 humanoid</strong> by 
                      integrating MuJoCo, RoboCasa, and cuRobo. The dataset provides synchronized RGB-D, contact-state, 
                      8D force–torque, and proprioceptive signals to support training and evaluation of <strong>foundation robot policies</strong>.
                    </p>
                  </td>
                </tr>

                <!-- Active Rainwater Removal System -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <img src="images/rainwater_system.jpg"
                             style="width:140%;margin-top:10px;border-radius:10px;border:1px solid #EEE;"
                             alt="Active Rainwater Removal System">
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <b>Active Rainwater Removal System</b>
                    <br>
                    Mechatronics Design Competition, Awarded — 2023
                    <p></p>
                    <p>
                      Developed a centrifugal rainwater removal device powered by a high-torque DC motor and a BTS7960 high-current H-bridge driver, 
                      integrating mechanical design with a custom motor control circuit.
                    </p>
                    <p>
                      As team lead, coordinated system integration and optimization to achieve fast water removal, low noise, and portable operation, 
                      contributing to the project’s award-winning performance.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:small;">
                      Template source: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
