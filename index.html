<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junsung Park</title>

    <meta name="author" content="Junsung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Header / Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Junsung Park
                    </p>

                    <p>
                      I am an undergraduate student in the
                      <a href="https://ece.snu.ac.kr/en">Department of Electrical and Computer Engineering</a>
                      at Seoul National University, with a minor in Mechanical Engineering.
                      I conduct research in the
                      <a href="http://rllab.snu.ac.kr/">Robot Learning Lab (RL LAB)</a> at SNU,
                      advised by Prof.
                      <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Songhwai Oh</a>.
                      My research interests include robot learning, vision–language–action models,
                      cross-embodiment policy transfer, and offline reinforcement learning.
                    </p>

                    <p style="text-align:center">
                      <a href="mailto:night1115@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                      <a href="data/JunsungPark_CV.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/junsung-park">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://github.com/jnsungp">Github</a>
                    </p>
                  </td>

                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/JunsungPark.jpg">
                      <img
                        style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;"
                        alt="profile photo"
                        src="images/JunsungPark.jpg">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <p>
                      My research aims to build robust and generalizable robot autonomy.
                      I focus on cross-embodiment policy adaptation, multimodal representation learning,
                      and learning-based control.
                      I am particularly interested in leveraging vision, depth, and contact-force signals
                      to develop manipulation policies that remain stable and reliable across diverse
                      robot embodiments.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Datasets Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Datasets</h2>
                    <p>
                      I build multimodal manipulation datasets for training and evaluating
                      foundation robot policies. My datasets emphasize high-quality contact modeling,
                      depth perception, force feedback, and trajectory diversity.
                    </p>
                  </td>
                </tr>

                <!-- G1 Dataset Card -->
                <tr>
                  <td style="padding:8px;width:100%;vertical-align:middle">
                    <span class="papertitle">
                      Unitree G1 Pick-and-Place Multimodal Dataset
                    </span>
                    <br>
                    <strong>Creator: Junsung Park</strong>
                    <br>
                    <em>Hosted on HuggingFace</em>
                    <br>
                    <a href="https://huggingface.co/jnsungp">HuggingFace Profile</a>

                    <p>
                      A large-scale multimodal simulation dataset designed for cross-embodiment
                      manipulation with the Unitree G1 humanoid.
                      Includes RGB, depth, contact-state, 8D force–torque signals, end-effector poses,
                      and cuRobo-generated trajectories built on a customized MuJoCo + RoboCasa pipeline.
                    </p>

                    <!-- Dataset Demo Video -->
                    <video
                      src="images/g1_dataset_demo.mp4"
                      autoplay
                      muted
                      loop
                      playsinline
                      style="width:70%;border-radius:12px;display:block;margin-left:auto;margin-right:auto;">
                    </video>

                  </td>
                </tr>

              </tbody>
            </table>

            <!-- Publications / Papers -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>

                <!-- Paper 1 -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <img
                      src="images/bicql-thumbnail.png"
                      style="width:100%;border-radius:10px;"
                      alt="BiCQL thumbnail">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <a href="#">
                      <span class="papertitle">
                        BiCQL-ML: A Bi-Level Conservative Q-Learning Framework
                        for Maximum Likelihood Inverse Reinforcement Learning
                      </span>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>arXiv (preprint)</em>
                    <br>
                    <a href="#">Paper (coming soon)</a>
                    <p>
                      A policy-free offline IRL framework that jointly optimizes rewards and
                      conservative Q-functions via bi-level optimization,
                      improving robustness and expert-likeness under dataset shift.
                    </p>
                  </td>
                </tr>

                <!-- Paper 2 -->
                <tr>
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <img
                      src="images/g1-modality.png"
                      style="width:100%;border-radius:10px;"
                      alt="G1 Modality">
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <a href="#">
                      <span class="papertitle">
                        Modality-Augmented Fine-Tuning of Foundation Robot Policies for Unitree G1
                      </span>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>In preparation (Target: RSS 2026)</em>
                    <br>
                    <a href="#">Paper (coming soon)</a>
                    <p>
                      A study on incorporating depth, binary contact, and 8D force feedback into
                      diffusion-based foundation robot policies for cross-embodiment manipulation
                      on the Unitree G1.
                    </p>
                  </td>
                </tr>

              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
