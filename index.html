<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junsung Park</title>

    <meta name="author" content="Junsung Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">

            <!-- Header / Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align:center;">
                      Junsung Park
                    </p>

                    <p>
                      I am an undergraduate student at <strong>Seoul National University (SNU)</strong>,
                      double majoring in <a href="https://ece.snu.ac.kr/en">Electrical and Computer Engineering</a>
                      and Mechanical Engineering.
                    </p>
                    
                    <p>
                      Currently, I am a research intern at the 
                      <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Robot Learning Laboratory (RLLAB)</a>,
                      advised by Prof. <a href="https://scholar.google.com/citations?user=N0ZCT00AAAAJ&hl=en">Songhwai Oh</a>.
                    </p>

                    <p>
                      My research goal is to build <strong>generalizable robot autonomy</strong> capable of 
                      <strong>robust interaction</strong> with humans and objects in dynamic, unstructured environments.
                    </p>

                    <p style="text-align:center">
                      <a href="mailto:night1115@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                      <a href="data/JunsungPark_CV.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://www.linkedin.com/in/junsung-park">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://github.com/jnsungp">Github</a>
                    </p>
                  </td>

                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/JunsungPark.jpg">
                      <img
                        style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;box-shadow:0 4px 8px rgba(0,0,0,0.2);"
                        alt="profile photo"
                        src="images/JunsungPark.jpg"
                        class="hoverZoomLink">
                    </a>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Research Interest -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research Interest</h2>
                    <p>
                      My primary interest lies in <strong>Robot Learning</strong>, utilizing <strong>Reinforcement Learning</strong> 
                      and <strong>Learning-based Control</strong> to build robust systems.
                      <br><br>
                      I explore <strong>Vision–Language–Action (VLA) models</strong> to enable generalizable autonomy,
                      with the ultimate goal of advancing <strong>Human-Robot Interaction</strong>.
                      I am also investigating <strong>Wearable Devices</strong> as a key interface to capture human intent.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Publications & Preprints -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Publications &amp; Preprints</h2>
                  </td>
                </tr>

                <!-- BiCQL-ML -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:120%;margin-top:9px;" muted autoplay loop playsinline>
                          <source src="images/BICQL.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://arxiv.org/abs/XXXXXXXX">
                      <b>BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood IRL</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>IEEE MIT URTC, Poster Presentation</em>, 2025
                    <br>
                    <a href="https://arxiv.org/abs/XXXXXXXX">[arXiv]</a>
                    <p></p>
                    <p>
                      We propose a <strong>policy-free offline Inverse Reinforcement Learning (IRL)</strong> framework that 
                      resolves reward ambiguity and distribution shift through a bi-level optimization structure
                      coupling conservative Q-learning with reward inference.
                    </p>
                  </td>
                </tr>

                <!-- Modality-Augmented Fine-Tuning -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:140%;margin-top:10px;" muted autoplay loop playsinline>
                          <source src="images/g1_inf.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="#">
                      <b>Modality-Augmented Fine-Tuning of Foundation Robot Policies</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>In Preparation</em>
                    <p></p>
                    <p>
                      Investigating modality-conditioned diffusion policies for cross-embodiment adaptation on the Unitree G1 humanoid.
                      The system integrates RGB-D, tactile states, and force–torque feedback to improve action stability and manipulation robustness.
                    </p>
                    <p>
                      Demonstrates significant performance gains via contact-state fusion and depth-aware conditioning strategies.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Projects -->
            <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td colspan="2" style="padding:8px;vertical-align:middle">
                    <h2>Projects</h2>
                  </td>
                </tr>

                <!-- Unitree G1 Dataset -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <video style="width:100%;margin-top:10px;" muted autoplay loop playsinline>
                          <source src="images/g1_dataset_demo.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>

                  <td style="padding:16px;width:70%;vertical-align:top">
                    <a href="https://huggingface.co/jnsungp">
                      <b>Unitree G1 Multimodal Manipulation Dataset</b>
                    </a>
                    <br>
                    <strong>Junsung Park</strong>
                    <br>
                    <em>Open Source Project &amp; Dataset</em>, 2024–
                    <br>
                    <a href="https://huggingface.co/jnsungp">[Dataset]</a> /
                    <a href="https://github.com/jnsungp">[Code]</a>
                    <p></p>
                    <p>
                      Built a large-scale multimodal dataset generation pipeline integrating MuJoCo, RoboCasa, and cuRobo.
                      Provides synchronized RGB-D, contact-state, force–torque, and proprioceptive data for training
                      foundation robot policies.
                    </p>
                  </td>
                </tr>

                <!-- Active Rainwater Removal System -->
                <tr>
                  <td style="padding:8px;width:30%;vertical-align:middle">
                    <div class="one">
                      <div class="two">
                        <img src="images/rainwater_system.jpg"
                             style="width:140%;margin-top:10px;border-radius:10px;border:1px solid #EEE;"
                             alt="Active Rainwater Removal System">
                      </div>
                    </div>
                  </td>

                  <td style="padding:16px;width:70%;vertical-align:top">
                    <b>Active Rainwater Removal System</b>
                    <br>
                    Mechatronics Design Competition, Awarded — 2023
                    <p></p>
                    <p>
                      Developed a centrifugal umbrella rainwater removal device using a high-torque DC motor
                      and a BTS7960 high-current driver.
                    </p>
                    <p>
                      Led system integration and control circuit design, achieving fast water removal,
                      low noise, and portable operation.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:0px">
                    <br>
                    <p style="text-align:right;font-size:small;">
                      Template source: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
